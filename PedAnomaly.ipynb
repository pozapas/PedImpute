{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''''\n",
    "# Pedestrian volume anomaly detection\n",
    "# Author:   Amir Rafe (amir.rafe@usu.edu)\n",
    "# File:     PedAnomaly.ipynb\n",
    "# Date:     Summer 2023\n",
    "# Version:  1.21  \n",
    "# About:    Methods to detect anomalies amont time series pedestrian volume data\n",
    "''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime as dt\n",
    "#\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "#\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "#\n",
    "from scipy import stats\n",
    "import ruptures as rpt\n",
    "#\n",
    "from sklearn.ensemble import IsolationForest, RandomForestRegressor\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.impute import KNNImputer\n",
    "from pyod.models.knn import KNN\n",
    "#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#\n",
    "from darts.models import RNNModel, TCNModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts import TimeSeries\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.utils.likelihood_models import GaussianLikelihood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load and preparing data\n",
    "\n",
    "# Load sample data\n",
    "df= pd.read_csv ('5323.csv')\n",
    "df['TIME1'] = pd.to_datetime(df['TIME1'])\n",
    "\n",
    "# Filter data based on COVID-19 phases in UTAH\n",
    "\n",
    "# Phase zero: pre-COVID19\n",
    "dfph0 = df.loc[(df['TIME1'] <= '2020-03-05')]\n",
    "\n",
    "# Phase one between March 2020 and April 2020\n",
    "start_date = pd.to_datetime('2020-03-06')\n",
    "end_date = pd.to_datetime('2020-04-30')\n",
    "dfph1 = df.loc[(df['TIME1'] >= start_date) & (df['TIME1'] <= end_date)]\n",
    "\n",
    "# Phase two between May 2020 and May 2021\n",
    "start_date = pd.to_datetime('2020-05-01')\n",
    "end_date = pd.to_datetime('2021-05-31')\n",
    "dfph2 = df.loc[(df['TIME1'] >= start_date) & (df['TIME1'] <= end_date )]\n",
    "\n",
    "# Phase three between June 2021 and Feburary 2022\n",
    "start_date = pd.to_datetime('2021-06-01')\n",
    "end_date = pd.to_datetime('2022-02-28')\n",
    "dfph3 = df.loc[(df['TIME1'] >= start_date) & (df['TIME1'] <= end_date)]\n",
    "\n",
    "# Phase four after March 2022\n",
    "dfph4 = df.loc[(df['TIME1'] >= '2022-03-01')]\n",
    "\n",
    "# Monthly data (choose start and end date based on your analysis type)\n",
    "start_date = pd.to_datetime('2022-03-06')\n",
    "end_date = pd.to_datetime('2022-04-06')\n",
    "dfM = df.loc[(df['TIME1'] >= start_date) & (df['TIME1'] <= end_date)]\n",
    "\n",
    "# Subset data for analysis\n",
    "data  = df[['TIME1' , 'PED', 'High Temp' , 'Low Temp', 'Precipitation' , 'Daily Case Rate / 100,000' , 'AQI']] \n",
    "data.set_index('TIME1', inplace=True)\n",
    "data = data.copy()\n",
    "data.loc[:, 'Average Temp'] = (data.loc[:, 'High Temp'] + data.loc[:, 'Low Temp']) / 2\n",
    "\n",
    "# Drop 'High Temp' and 'Low Temp' columns\n",
    "data = data.drop(['High Temp', 'Low Temp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decomposition, Seasonality and Stationarity\n",
    "\n",
    "df_clean = dfM.copy() # choose the desired subdatset\n",
    "df_clean = df_clean.dropna()\n",
    "\n",
    "# Perform the decomposition\n",
    "decomposition = seasonal_decompose(df_clean['PED'], period=12)\n",
    "decomposition.plot()\n",
    "plt.show()\n",
    "\n",
    "# Seasonality check\n",
    "autocorrelation_plot(df_clean['PED'])\n",
    "plt.title('Autocorrelation of Pedestrian Volume')\n",
    "plt.show()\n",
    "\n",
    "# Check stationarity of the variables \n",
    "dataNa = data.dropna()\n",
    "def check_stationarity(data):\n",
    "    stationary = True\n",
    "    for col in data.columns:\n",
    "        result = adfuller(data[col])\n",
    "        p_value = result[1]\n",
    "        if p_value > 0.05:\n",
    "            print(f\"Warning: {col} is non-stationary (p-value = {p_value})\")\n",
    "            stationary = False\n",
    "    return stationary\n",
    "check_stationarity(dataNa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Change points and VAR analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Detect change points \n",
    "\n",
    "# Choose a method from the following lists:\n",
    "\n",
    "    #'Least absolute deviation': 'l1'\n",
    "    #'Least squared deviation_2': 'l2',\n",
    "    #'Gaussian process change': 'normal',\n",
    "    #'Kernelized mean change': 'rbf',\n",
    "    #'Autoregressive model change': 'ar'\n",
    "    #rpt.BottomUp\n",
    "    #rpt.Window\n",
    "    #rpt.Dynp\n",
    "    #rpt.Binseg\n",
    "\n",
    "data1  = df[['TIME1' , 'PED']]\n",
    "data1.set_index('TIME1', inplace=True)\n",
    "algo = rpt.Binseg(model=\"normal\").fit(data1)\n",
    "result = algo.predict(n_bkps=15)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=data1.index, y=data1['PED'], mode='lines', name='Pedestrian'))\n",
    "for cp in result:\n",
    "    if cp < len(data1.index) - 1:\n",
    "        fig.add_shape(type='line', x0=data1.index[cp], y0=0, x1=data1.index[cp], y1=max(data1['PED']), line=dict(color='red', width=1))\n",
    "\n",
    "change_point_dates = []\n",
    "for cp in result:\n",
    "    if cp < len(data1.index) - 1:\n",
    "        change_point_date = data1.index[cp]\n",
    "        change_point_dates.append(change_point_date)\n",
    "        fig.add_shape(type='line', x0=change_point_date, y0=0, x1=change_point_date, y1=max(data1['PED']), line=dict(color='red', width=1))\n",
    "\n",
    "print(\"Change Point Dates:\")\n",
    "print(change_point_dates)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VAR analysis between each change points and craete AD (Analysis Duration)\n",
    "\n",
    "# RMSE function\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "# Data frame start and end point\n",
    "start_date = pd.Timestamp('2018-01-01')\n",
    "end_date = pd.Timestamp('2022-12-31')\n",
    "\n",
    "# Create subset dataframe between change points\n",
    "subset_dataframes = []\n",
    "previous_date = start_date\n",
    "\n",
    "for change_point_date in change_point_dates:\n",
    "    if change_point_date <= start_date or change_point_date >= end_date:\n",
    "        continue\n",
    "    \n",
    "    # Check if the time difference is less than one month\n",
    "    if (change_point_date - previous_date).days > 30:\n",
    "        subset_data = data.loc[previous_date:change_point_date - pd.DateOffset(days=1)]\n",
    "        subset_dataframes.append(subset_data)\n",
    "        previous_date = change_point_date\n",
    "\n",
    "# Add the last subset from the last change point to the end of the data\n",
    "if previous_date < end_date:\n",
    "    subset_data = data.loc[previous_date:end_date]\n",
    "    subset_dataframes.append(subset_data)\n",
    "\n",
    "# VAR analysis\n",
    "for i, subset_data in enumerate(subset_dataframes):\n",
    "    print(f\"Subset {i+1}:\")\n",
    "    \n",
    "    # Get the start date of the subset\n",
    "    start_date = subset_data.index[0]\n",
    "\n",
    "    # Set a specific date range for the subset data\n",
    "    subset_data.index = pd.date_range(start=start_date, periods=len(subset_data), freq='D')\n",
    "\n",
    "    # Remove rows with missing values\n",
    "    subset_data = subset_data.dropna()\n",
    "\n",
    "    # Difference the data\n",
    "    subset_data_diff = subset_data - subset_data.shift(5) # based on seasonality\n",
    "    subset_data_diff = subset_data_diff.dropna()\n",
    "\n",
    "    # Split the data into training and testing sets (using an 80-20 split)\n",
    "    train_size = int(len(subset_data_diff) * 0.8)\n",
    "    train_data = subset_data_diff[:train_size]\n",
    "    test_data = subset_data_diff[train_size:]\n",
    "\n",
    "    # Fit the VAR model\n",
    "    model = VAR(train_data)\n",
    "\n",
    "    try:\n",
    "        # Determine the optimal lag order using AIC or BIC\n",
    "        lag_order = model.select_order()\n",
    "    except KeyError:\n",
    "        print(\"Error: Could not calculate AIC for model order selection.\")\n",
    "        continue\n",
    "\n",
    "    # Fit the VAR model with the chosen lag order\n",
    "    results = model.fit(lag_order.aic)\n",
    "\n",
    "    # Print the summary of the model\n",
    "    print(results.summary())\n",
    "\n",
    "    # Evaluate the model on the testing data\n",
    "    forecast = results.forecast(y=train_data.values, steps=len(test_data))\n",
    "    forecast_df = pd.DataFrame(forecast, index=test_data.index, columns=test_data.columns)\n",
    "\n",
    "    # Calculate RMSE for each variable\n",
    "    for col in forecast_df.columns:\n",
    "        print(f\"RMSE for {col}: {rmse(forecast_df[col], test_data[col])}\")\n",
    "\n",
    "    print(check_stationarity(subset_data_diff))\n",
    "    \n",
    "    # Get the p-values for the PED variable\n",
    "    p_values = results.pvalues['PED']\n",
    "\n",
    "    # Set the significance level\n",
    "    alpha = 0.05\n",
    "\n",
    "    # List of variable names\n",
    "    variables = ['Daily Case Rate / 100,000', 'AQI', 'Average Temp', 'Precipitation']\n",
    "\n",
    "    # Loop through each variable\n",
    "    for variable in variables:\n",
    "        \n",
    "        # Find the significant variables and their lags\n",
    "        significant_vars = []\n",
    "        for lag in range(1, lag_order.aic + 1):\n",
    "            if p_values[f'L{lag}.{variable}'] < alpha:\n",
    "                significant_vars.append((f'L{lag}.{variable}', lag))\n",
    "\n",
    "        # Print the significant variables and their lags\n",
    "        if significant_vars:\n",
    "            print(\"Significant variables and their lags:\")\n",
    "            for var, lag in significant_vars:\n",
    "                print(f\"{var} at lag {lag}\")\n",
    "        else:\n",
    "            print(f\"No significant variables found for the PED equation with {variable}.\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inject synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inject synthetic outliers\n",
    "\n",
    "# Create a copy of the original DataFrame to store the outliers\n",
    "data1 = data1.sort_index()\n",
    "data_outliers = data1.copy()\n",
    "\n",
    "# Add a column 'is_outlier' initialized with zeros\n",
    "data_outliers['is_outlier'] = 0\n",
    "\n",
    "# Options to generate steady outliers\n",
    "#random.seed(42)\n",
    "#np.random.seed(42)\n",
    "\n",
    "# Store dates of the change points, adding the first and the last date of the dataframe\n",
    "# Also filter out any invalid indices from the result list\n",
    "result_filtered = [i for i in result if i < len(data1)]\n",
    "changepoints = [data1.index[0]] + [data1.index[i] for i in result_filtered] + [data1.index[-1]]\n",
    "\n",
    "# Define outliers in each segment\n",
    "num_outliers_per_segment = random.choice([2, 5])\n",
    "\n",
    "# Define the range of increase or decrease the values by percentage\n",
    "percentage_change = 0.60 \n",
    "percentage_change2 = 0.85\n",
    "\n",
    "# For each pair of change points\n",
    "for i in range(len(changepoints) - 1):\n",
    "    # Check if the period is between June 15, 2019 and September 8, 2019 (period of the block missing value for the sample data)\n",
    "    if (changepoints[i] > pd.to_datetime('2019-06-15') and \n",
    "        changepoints[i+1] < pd.to_datetime('2019-09-08')):\n",
    "        continue\n",
    "\n",
    "    # Generate outliers\n",
    "    for _ in range(num_outliers_per_segment):\n",
    "        # Select a random date between the current and next change point\n",
    "        outlier_date = np.random.choice(pd.date_range(changepoints[i], changepoints[i+1]))\n",
    "\n",
    "        # Generate an outlier by increasing or decreasing the value at the chosen date by a specific percentage\n",
    "        increase_or_decrease = 1 + 1 * random.choice([percentage_change, percentage_change2])\n",
    "        outlier = data_outliers.loc[outlier_date, 'PED'] * increase_or_decrease\n",
    "\n",
    "        # Replace the value at the chosen date with the generated outlier in the outlier DataFrame\n",
    "        data_outliers.loc[outlier_date, 'PED'] = outlier\n",
    "\n",
    "        # Mark the generated outlier in 'is_outlier' column\n",
    "        data_outliers.loc[outlier_date, 'is_outlier'] = 1\n",
    "\n",
    "# Create a separate DataFrame for outliers by filtering only the modified rows\n",
    "data_only_outliers = data_outliers[data_outliers['is_outlier'] == 1]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add pedestrian volume to the plot\n",
    "fig.add_trace(go.Scatter(x=data_outliers.index, y=data_outliers['PED'], mode='lines', name='Data with injected outliers'))\n",
    "fig.add_trace(go.Scatter(x=data1.index, y=data1['PED'], mode='lines', name='Original data'))\n",
    "# Add outliers to the plot\n",
    "fig.add_trace(go.Scatter(x=data_only_outliers.index, y=data_only_outliers['PED'], mode='markers', name='Synthetic outliers', marker=dict(color='orange', size=10)))\n",
    "fig.update_layout(\n",
    "                  xaxis=dict(title='Date'),\n",
    "                yaxis=dict(title='Pedestrian volume'))\n",
    "# Add change points to the plot\n",
    "for cp in result_filtered:\n",
    "    fig.add_shape(type='line', x0=data1.index[cp], y0=0, x1=data1.index[cp], y1=max(data_outliers['PED']), line=dict(color='gray', width=1))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Isolation Forest method\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data_outliers = data_outliers.dropna()\n",
    "\n",
    "# Reshape the values column to 2D array as needed by the model\n",
    "values = data_outliers['PED'].values.reshape(-1, 1)\n",
    "\n",
    "# Define the model\n",
    "model = IsolationForest(contamination=0.1)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(values)\n",
    "\n",
    "# Predict the outliers\n",
    "outliers = model.predict(values)\n",
    "\n",
    "# Replace outliers (-1) with NaN\n",
    "data_outliers['PED'][outliers == -1] = np.nan\n",
    "\n",
    "# Reset index to plot data using Plotly \n",
    "data_outliers_reset = data_outliers.reset_index()\n",
    "\n",
    "# Plot the final dataset\n",
    "fig = px.line(data_outliers_reset, x='TIME1', y='PED', title='Time Series with Outliers Replaced by NaN')\n",
    "fig.add_trace(go.Scatter(x=data1.index, y=data1['PED'], mode='lines', name='Data with injected outliers'))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN method\n",
    "\n",
    "# Replace initial NaN values with a placeholder value\n",
    "data_outliers.fillna(-99999, inplace=True)\n",
    "\n",
    "# Create a 2D array of the 'PED' column values\n",
    "values = data_outliers['PED'].values.reshape(-1,1)\n",
    "\n",
    "# Define the model\n",
    "clf = KNN(contamination=0.1)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(values)\n",
    "\n",
    "# Get the prediction labels of the training data\n",
    "y_train_pred = clf.labels_ \n",
    "\n",
    "# Outlier points are predicted as 1 while inliers are predicted as 0\n",
    "data_outliers['Outlier'] = y_train_pred\n",
    "\n",
    "# Replace outlier points and initial NaN values (placeholder values) with NaN\n",
    "data_outliers.loc[data_outliers['Outlier'] == 1, 'PED'] = np.nan\n",
    "data_outliers.replace(-99999, np.nan, inplace=True)\n",
    "\n",
    "# Drop the 'Outlier' column\n",
    "data_outliers.drop('Outlier', axis=1, inplace=True)\n",
    "\n",
    "# Create the initial line trace\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add outliers to the plot\n",
    "fig.add_trace(go.Scatter(x=data_only_outliers.index, y=data_only_outliers['PED'], mode='markers', name='Synthetic outliers', marker=dict(color='orange', size=10)))\n",
    "fig.add_trace(go.Scatter(x=data_outliers_reset['TIME1'], y=data_outliers_reset['PED'], mode='lines', name='Cleaned data', line=dict(color='#636EFA'))) \n",
    "\n",
    "for cp in result_filtered:\n",
    "    # Add a line with go.Scatter\n",
    "    fig.add_trace(go.Scatter(x=[data1.index[cp], data1.index[cp]], y=[0, max(data_outliers['PED'])], mode='lines', line=dict(color='gray', width=1), showlegend=False))\n",
    "\n",
    "# Use the update_layout() function to update axis titles\n",
    "fig.update_layout(\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Pedestrian volume',\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DBSCAN method\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data_outlierss = data_outliers.dropna()\n",
    "\n",
    "# Reshape the values column to 2D array as needed by the model\n",
    "values = data_outlierss['PED'].values.reshape(-1, 1)\n",
    "\n",
    "# Define the model\n",
    "model = DBSCAN(min_samples=11, eps=0.5)\n",
    "\n",
    "# Fit the model and predict outliers\n",
    "outliers = model.fit_predict(values)\n",
    "\n",
    "# Replace outliers (-1) with NaN\n",
    "data_outlierss['PED'][outliers == -1] = np.nan\n",
    "\n",
    "# Reset index to plot data using Plotly \n",
    "data_outliers_reset = data_outlierss.reset_index()\n",
    "\n",
    "# Create the initial line trace\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add outliers to the plot\n",
    "fig.add_trace(go.Scatter(x=data_only_outliers.index, y=data_only_outliers['PED'], mode='markers', name='Synthetic outliers', marker=dict(color='orange', size=10)))\n",
    "fig.add_trace(go.Scatter(x=data_outliers_reset['TIME1'], y=data_outliers_reset['PED'], mode='lines', name='Cleaned data', line=dict(color='#636EFA'))) \n",
    "\n",
    "for cp in result_filtered:\n",
    "    # Add a line with go.Scatter\n",
    "    fig.add_trace(go.Scatter(x=[data1.index[cp], data1.index[cp]], y=[0, max(data_outliers['PED'])], mode='lines', line=dict(color='gray', width=1), showlegend=False))\n",
    "\n",
    "# Use the update_layout() function to update axis titles\n",
    "fig.update_layout(\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Pedestrian volume',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest method\n",
    "\n",
    "# Create a mask for the period we are interested in\n",
    "mask = (data_outlierss.index > '2018-01-01') & (data_outlierss.index <= '2022-12-31')\n",
    "\n",
    "# Create a new DataFrame for that period\n",
    "df_period = data_outlierss.loc[mask].copy()\n",
    "\n",
    "# Convert date index to ordinal for training purpose\n",
    "df_period['date_ordinal'] = df_period.index.map(dt.toordinal)\n",
    "\n",
    "# Prepare data for imputation\n",
    "X = df_period[['date_ordinal', 'PED']].values\n",
    "\n",
    "# Split data into data with known 'PED' and data with unknown 'PED'\n",
    "known = X[~np.isnan(X[:, 1])]  # rows where 'PED' is known\n",
    "unknown = X[np.isnan(X[:, 1])]  # rows where 'PED' is unknown\n",
    "\n",
    "# Create RandomForestRegressor\n",
    "regressor = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "\n",
    "# Fit the regressor and predict\n",
    "if unknown.size > 0:  # If there are any unknown 'PED' values\n",
    "    regressor.fit(known[:, 0].reshape(-1, 1), known[:, 1])  # Fit the regressor\n",
    "    predictions = regressor.predict(unknown[:, 0].reshape(-1, 1))  # Predict the unknown 'PED' values\n",
    "    X[np.isnan(X[:, 1]), 1] = predictions  # Fill the unknown 'PED' values with the predictions\n",
    "\n",
    "# Update 'PED' in df_period with imputed data\n",
    "df_period['PED'] = X[:, 1]\n",
    "\n",
    "# Plot the original data and imputed data\n",
    "fig = go.Figure()\n",
    "\n",
    "# Imputed data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_period.index,\n",
    "    y=df_period['PED'],\n",
    "    mode='lines',\n",
    "    name='Imputed data',\n",
    "    line=dict(color='red')\n",
    "))\n",
    "\n",
    "# Original data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data_outlierss.index,\n",
    "    y=data_outlierss['PED'],\n",
    "    mode='lines',\n",
    "    line=dict(color='#636EFA'),\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "# Update x-axis range\n",
    "#fig.update_xaxes(range=['2018-08-21', '2019-04-01'])\n",
    "\n",
    "# Use the update_layout() function to update axis titles\n",
    "fig.update_layout(\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Pedestrian volume',\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNN method\n",
    "\n",
    "# Create a mask for the period we are interested in\n",
    "mask = (data_outlierss.index > '2018-01-01') & (data_outlierss.index <= '2022-12-31')\n",
    "\n",
    "# Create a new DataFrame for that period\n",
    "df_period = data_outlierss.loc[mask]\n",
    "\n",
    "# Convert date index to ordinal for training purpose\n",
    "df_period['date_ordinal'] = df_period.index.map(dt.toordinal)\n",
    "\n",
    "# Prepare data for imputation\n",
    "X = df_period[['date_ordinal', 'PED']].values\n",
    "\n",
    "# Create KNN imputer\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "\n",
    "# Perform imputation\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Update 'value' in df_period with imputed data\n",
    "df_period['PED'] = X_imputed[:, 1]\n",
    "\n",
    "# Plot the original data and imputed data\n",
    "fig = go.Figure()\n",
    "\n",
    "# Imputed data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_period.index,\n",
    "    y=df_period['PED'],\n",
    "    mode='lines',\n",
    "    name='Imputed data',\n",
    "    line=dict(color='red')\n",
    "))\n",
    "\n",
    "# Original data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data_outlierss.index,\n",
    "    y=data_outlierss['PED'],\n",
    "    mode='lines',\n",
    "    line=dict(color='#636EFA'),\n",
    "    showlegend=False\n",
    "))\n",
    "# Update x-axis range\n",
    "#fig.update_xaxes(range=['2018-08-21', '2019-04-01'])\n",
    "\n",
    "# Use the update_layout() function to update axis titles\n",
    "fig.update_layout(\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Pedestrian volume',\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM method - Univariate imputation\n",
    "\n",
    "# Define LSTM Model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n",
    "                            torch.zeros(1,1,self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq), 1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]\n",
    "# Prepare data\n",
    "data1 = data.reset_index()\n",
    "data1['TIME1'] = pd.to_datetime(data1['TIME1'])\n",
    "data1 = data1[data1['TIME1'] <= '2019-06-11']\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data_normalized = scaler.fit_transform(data1['PED'].values.reshape(-1, 1))\n",
    "\n",
    "# Transform data to tensors\n",
    "data_normalized = torch.FloatTensor(data_normalized).view(-1)\n",
    "\n",
    "# Define a method to create in/out sequences\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input_data[i:i+tw]\n",
    "        train_label = input_data[i+tw:i+tw+1]\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "    return inout_seq\n",
    "\n",
    "# Prepare sequences\n",
    "train_window = 20  \n",
    "train_inout_seq = create_inout_sequences(data_normalized, train_window)\n",
    "\n",
    "# Initialize the model, define loss function and optimizer\n",
    "model = LSTM(1, 20, 1)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_inout_seq:\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "        y_pred = model(seq)\n",
    "\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "fut_pred = 91\n",
    "test_inputs = data_normalized[-train_window:].tolist()\n",
    "\n",
    "for i in range(fut_pred):\n",
    "    seq = torch.FloatTensor(test_inputs[-train_window:])\n",
    "    with torch.no_grad():\n",
    "        model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
    "        test_inputs.append(model(seq).item())\n",
    "\n",
    "# Invert normalization\n",
    "actual_predictions = scaler.inverse_transform(np.array(test_inputs[train_window:]).reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted data\n",
    "plt.plot(data1['PED'].values, label='actual')\n",
    "plt.plot(range(len(data1['PED']), len(data1['PED']) + fut_pred), actual_predictions, label='forecast')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM method - Multivariate imputation\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n",
    "                            torch.zeros(1,1,self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq) ,1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]\n",
    "\n",
    "# Prepare data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Include the time series and exogenous variables\n",
    "data1 = data.copy()\n",
    "data1=data1.reset_index()\n",
    "data1['TIME1'] = pd.to_datetime(data1['TIME1'])\n",
    "data1 = data1[data1['TIME1'] <= '2019-06-11']\n",
    "data1 = data1[['PED', 'AQI', #'Daily Case Rate / 100,000'\n",
    "               'Average Temp', 'Precipitation']]\n",
    "\n",
    "# Normalize the dataset\n",
    "data_normalized = scaler.fit_transform(data1)\n",
    "\n",
    "# Transform data to tensors\n",
    "data_normalized = torch.FloatTensor(data_normalized)\n",
    "\n",
    "# Define a method to create in/out sequences\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input_data[i:i+tw]\n",
    "        train_label = input_data[i+tw:i+tw+1, 0]  \n",
    "        inout_seq.append((train_seq, train_label))\n",
    "    return inout_seq\n",
    "\n",
    "# Prepare sequences\n",
    "train_window = 20  \n",
    "train_inout_seq = create_inout_sequences(data_normalized, train_window)\n",
    "\n",
    "# Initialize the model, define loss function and optimizer\n",
    "model = LSTM(4, 20, 1)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_inout_seq:\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "        y_pred = model(seq)\n",
    "\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "fut_pred = 91\n",
    "# Take the last `train_window` sets of data points\n",
    "test_inputs = data_normalized[-train_window:].tolist()\n",
    "test_inputs = test_inputs + [[0]*4 for _ in range(fut_pred)]\n",
    "\n",
    "for i in range(fut_pred):\n",
    "    seq = torch.FloatTensor(test_inputs[i:i+train_window])\n",
    "    with torch.no_grad():\n",
    "        model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
    "        test_inputs[i+train_window][0] = model(seq).item()  \n",
    "        \n",
    "# Extract the `fut_pred` predictions made by the model\n",
    "predictions = test_inputs[-fut_pred:]\n",
    "\n",
    "# Invert normalization\n",
    "actual_predictions = scaler.inverse_transform(predictions)[:, 0]  \n",
    "\n",
    "# Plot the actual vs predicted data\n",
    "plt.plot(data1['PED'].values, label='actual')\n",
    "plt.plot(range(len(data1['PED']), len(data1['PED']) + fut_pred), actual_predictions, label='forecast')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRU method - Univariate imputation\n",
    "\n",
    "# Define GRU Model\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.gru = nn.GRU(input_size, hidden_layer_size)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        self.hidden_cell = torch.zeros(1,1,self.hidden_layer_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        gru_out, self.hidden_cell = self.gru(input_seq.view(len(input_seq), 1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(gru_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]\n",
    "\n",
    "# Prepare data\n",
    "data1 = data.reset_index()\n",
    "data1['TIME1'] = pd.to_datetime(data1['TIME1'])\n",
    "data1 = data1[data1['TIME1'] <= '2019-06-11']\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data_normalized = scaler.fit_transform(data1['PED'].values.reshape(-1, 1))\n",
    "\n",
    "# Transform data to tensors\n",
    "data_normalized = torch.FloatTensor(data_normalized).view(-1)\n",
    "\n",
    "# Define a method to create in/out sequences\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input_data[i:i+tw]\n",
    "        train_label = input_data[i+tw:i+tw+1]\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "    return inout_seq\n",
    "\n",
    "# Prepare sequences\n",
    "train_window = 20  \n",
    "train_inout_seq = create_inout_sequences(data_normalized, train_window)\n",
    "\n",
    "# Initialize the model, define loss function and optimizer\n",
    "model = GRU(1, 20, 1)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_inout_seq:\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = torch.zeros(1, 1, model.hidden_layer_size)\n",
    "\n",
    "        y_pred = model(seq)\n",
    "\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "fut_pred = 91\n",
    "test_inputs = data_normalized[-train_window:].tolist()\n",
    "\n",
    "for i in range(fut_pred):\n",
    "    seq = torch.FloatTensor(test_inputs[-train_window:])\n",
    "    with torch.no_grad():\n",
    "        model.hidden = torch.zeros(1, 1, model.hidden_layer_size)\n",
    "        test_inputs.append(model(seq).item())\n",
    "\n",
    "# Invert normalization\n",
    "actual_predictions = scaler.inverse_transform(np.array(test_inputs[train_window:]).reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted data\n",
    "plt.plot(data1['PED'].values, label='actual')\n",
    "plt.plot(range(len(data1['PED']), len(data1['PED']) + fut_pred), actual_predictions, label='forecast')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRU method - Multivariate imputation\n",
    "\n",
    "# Define GRU Model\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_layer_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = torch.zeros(1,1,self.hidden_layer_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        gru_out, self.hidden_cell = self.gru(input_seq.view(len(input_seq) ,1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(gru_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]\n",
    "\n",
    "# Prepare data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Include the time series and exogenous variables\n",
    "data1 = data.copy()\n",
    "data1=data1.reset_index()\n",
    "data1['TIME1'] = pd.to_datetime(data1['TIME1'])\n",
    "data1 = data1[data1['TIME1'] <= '2019-06-11']\n",
    "data1 = data1[['PED', 'AQI', #'Daily Case Rate / 100,000'\n",
    "               'Average Temp', 'Precipitation']]\n",
    "\n",
    "# Normalize the dataset\n",
    "data_normalized = scaler.fit_transform(data1)\n",
    "\n",
    "# Transform data to tensors\n",
    "data_normalized = torch.FloatTensor(data_normalized)\n",
    "\n",
    "# Define a method to create in/out sequences\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input_data[i:i+tw]\n",
    "        train_label = input_data[i+tw:i+tw+1, 0]  \n",
    "        inout_seq.append((train_seq, train_label))\n",
    "    return inout_seq\n",
    "\n",
    "# Prepare sequences\n",
    "train_window = 20  \n",
    "train_inout_seq = create_inout_sequences(data_normalized, train_window)\n",
    "\n",
    "# Initialize the model, define loss function and optimizer\n",
    "model = GRU(4, 20, 1)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_inout_seq:\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = torch.zeros(1, 1, model.hidden_layer_size)\n",
    "\n",
    "        y_pred = model(seq)\n",
    "\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "fut_pred = 91\n",
    "# Take the last `train_window` sets of data points\n",
    "test_inputs = data_normalized[-train_window:].tolist()\n",
    "test_inputs = test_inputs + [[0]*4 for _ in range(fut_pred)]\n",
    "\n",
    "for i in range(fut_pred):\n",
    "    seq = torch.FloatTensor(test_inputs[i:i+train_window])\n",
    "    with torch.no_grad():\n",
    "        model.hidden = torch.zeros(1, 1, model.hidden_layer_size)\n",
    "        test_inputs[i+train_window][0] = model(seq).item()  \n",
    "\n",
    "# Extract the `fut_pred` predictions made by the model\n",
    "predictions = test_inputs[-fut_pred:]\n",
    "\n",
    "# Invert normalization\n",
    "actual_predictions = scaler.inverse_transform(predictions)[:, 0]  \n",
    "\n",
    "# Plot the actual vs predicted data\n",
    "plt.plot(data1['PED'].values, label='actual')\n",
    "plt.plot(range(len(data1['PED']), len(data1['PED']) + fut_pred), actual_predictions, label='forecast')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN method - Univariate imputation\n",
    "\n",
    "# Define RNN Model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_layer_size)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        self.hidden_cell = torch.zeros(1,1,self.hidden_layer_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        rnn_out, self.hidden_cell = self.rnn(input_seq.view(len(input_seq), 1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(rnn_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]\n",
    "\n",
    "# Prepare data\n",
    "data1 = data.reset_index()\n",
    "data1['TIME1'] = pd.to_datetime(data1['TIME1'])\n",
    "data1 = data1[data1['TIME1'] <= '2019-06-11']\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data_normalized = scaler.fit_transform(data1['PED'].values.reshape(-1, 1))\n",
    "\n",
    "# Transform data to tensors\n",
    "data_normalized = torch.FloatTensor(data_normalized).view(-1)\n",
    "\n",
    "# Define a method to create in/out sequences\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input_data[i:i+tw]\n",
    "        train_label = input_data[i+tw:i+tw+1]\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "    return inout_seq\n",
    "\n",
    "# Prepare sequences\n",
    "train_window = 20  \n",
    "train_inout_seq = create_inout_sequences(data_normalized, train_window)\n",
    "\n",
    "# Initialize the model, define loss function and optimizer\n",
    "model = RNN(1, 20, 1)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_inout_seq:\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = torch.zeros(1, 1, model.hidden_layer_size)\n",
    "\n",
    "        y_pred = model(seq)\n",
    "\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "fut_pred = 91\n",
    "test_inputs = data_normalized[-train_window:].tolist()\n",
    "\n",
    "for i in range(fut_pred):\n",
    "    seq = torch.FloatTensor(test_inputs[-train_window:])\n",
    "    with torch.no_grad():\n",
    "        model.hidden = torch.zeros(1, 1, model.hidden_layer_size)\n",
    "        test_inputs.append(model(seq).item())\n",
    "\n",
    "# Invert normalization\n",
    "actual_predictions = scaler.inverse_transform(np.array(test_inputs[train_window:]).reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted data\n",
    "plt.plot(data1['PED'].values, label='actual')\n",
    "plt.plot(range(len(data1['PED']), len(data1['PED']) + fut_pred), actual_predictions, label='forecast')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN method - Multivariate imputation\n",
    "\n",
    "# Define RNN Model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_layer_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = torch.zeros(1,1,self.hidden_layer_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        rnn_out, self.hidden_cell = self.rnn(input_seq.view(len(input_seq) ,1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(rnn_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]\n",
    "\n",
    "# Prepare data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Include the time series and exogenous variables\n",
    "data1 = data.copy()\n",
    "data1=data1.reset_index()\n",
    "data1['TIME1'] = pd.to_datetime(data1['TIME1'])\n",
    "data1 = data1[data1['TIME1'] <= '2019-06-11']\n",
    "data1 = data1[['PED', 'AQI', #'Daily Case Rate / 100,000'\n",
    "               'Average Temp', 'Precipitation']]\n",
    "\n",
    "# Normalize the dataset\n",
    "data_normalized = scaler.fit_transform(data1)\n",
    "\n",
    "# Transform data to tensors\n",
    "data_normalized = torch.FloatTensor(data_normalized)\n",
    "\n",
    "# Define a method to create in/out sequences\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input_data[i:i+tw]\n",
    "        train_label = input_data[i+tw:i+tw+1, 0]  \n",
    "        inout_seq.append((train_seq, train_label))\n",
    "    return inout_seq\n",
    "\n",
    "# Prepare sequences\n",
    "train_window = 20  \n",
    "train_inout_seq = create_inout_sequences(data_normalized, train_window)\n",
    "\n",
    "# Initialize the model, define loss function and optimizer\n",
    "model = RNN(4, 20, 1)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_inout_seq:\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = torch.zeros(1, 1, model.hidden_layer_size)\n",
    "\n",
    "        y_pred = model(seq)\n",
    "\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "fut_pred = 91\n",
    "# Take the last `train_window` sets of data points\n",
    "test_inputs = data_normalized[-train_window:].tolist()\n",
    "test_inputs = test_inputs + [[0]*4 for _ in range(fut_pred)]\n",
    "\n",
    "for i in range(fut_pred):\n",
    "    seq = torch.FloatTensor(test_inputs[i:i+train_window])\n",
    "    with torch.no_grad():\n",
    "        model.hidden = torch.zeros(1, 1, model.hidden_layer_size)\n",
    "        test_inputs[i+train_window][0] = model(seq).item()  \n",
    "\n",
    "# Extract the `fut_pred` predictions made by the model\n",
    "predictions = test_inputs[-fut_pred:]\n",
    "\n",
    "# Invert normalization\n",
    "actual_predictions = scaler.inverse_transform(predictions)[:, 0]  \n",
    "\n",
    "# Plot the actual vs predicted data\n",
    "plt.plot(data1['PED'].values, label='actual')\n",
    "plt.plot(range(len(data1['PED']), len(data1['PED']) + fut_pred), actual_predictions, label='forecast')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM method - Univariate imputation using darts\n",
    "\n",
    "# Initialize the transformer and the model\n",
    "transformer = Scaler()\n",
    "model = RNNModel(\n",
    "    model=\"LSTM\",\n",
    "    hidden_dim=20,\n",
    "    dropout=0,\n",
    "    batch_size=16,\n",
    "    n_epochs=50,\n",
    "    optimizer_kwargs={\"lr\": 1e-3},\n",
    "    random_state=0,\n",
    "    training_length=50,\n",
    "    input_chunk_length=20,\n",
    "    likelihood=GaussianLikelihood(),\n",
    ")\n",
    "# Prepare the data\n",
    "data1 = data.copy()\n",
    "data1=data1.reset_index()\n",
    "data1['TIME1'] = pd.to_datetime(data1['TIME1'])\n",
    "data1 = data1[data1['TIME1'] <= '2019-06-11']\n",
    "data2 = data.copy()\n",
    "data2=data2.reset_index()\n",
    "data2['TIME1'] = pd.to_datetime(data2['TIME1'])\n",
    "data2 = data2[data2['TIME1'] >= '2019-09-10']\n",
    "data2.set_index('TIME1', inplace=True)\n",
    "data_ts = TimeSeries.from_dataframe(data1, 'TIME1', 'PED')\n",
    "data_scaled = transformer.fit_transform(data_ts)\n",
    "\n",
    "# Create year and month covariate series\n",
    "year_series = datetime_attribute_timeseries(\n",
    "    pd.date_range(start=data_ts.start_time(), freq=data_ts.freq_str, periods=len(data_ts)+91),\n",
    "    attribute=\"year\",\n",
    "    one_hot=False,\n",
    ")\n",
    "year_series = Scaler().fit_transform(year_series)\n",
    "month_series = datetime_attribute_timeseries(\n",
    "    year_series, attribute=\"month\", one_hot=True\n",
    ")\n",
    "covariates = year_series.stack(month_series)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(data_scaled, future_covariates=covariates)\n",
    "\n",
    "# Predict the next n days\n",
    "forecast = model.predict(n=91, future_covariates=covariates)\n",
    "\n",
    "# Re-scale the values to the original scale\n",
    "forecast = transformer.inverse_transform(forecast)\n",
    "\n",
    "# Create a single trace for the entire dataset\n",
    "trace_all = go.Scatter(\n",
    "    x=np.concatenate([data_ts.time_index, forecast.time_index, data2.index]),\n",
    "    y=np.concatenate([data_ts.univariate_values(), forecast.univariate_values(), data2['PED'].values]),\n",
    "    mode='lines',\n",
    "    name='All',\n",
    "    line=dict(color='red'),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Create separate traces for each segment with desired colors\n",
    "trace1 = go.Scatter(\n",
    "    x=data_ts.time_index, \n",
    "    y=data_ts.univariate_values(),\n",
    "    mode='lines',\n",
    "    line=dict(color='#636EFA'),\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x=forecast.time_index, \n",
    "    y=forecast.univariate_values(),\n",
    "    mode='lines',\n",
    "    name='Imputed data',\n",
    "    line=dict(color='red'),\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "    x=data2.index,\n",
    "    y=data2['PED'],\n",
    "    mode='lines',\n",
    "    line=dict(color='#636EFA'),\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='Pedestrian volume'),\n",
    ")\n",
    "\n",
    "# Create the figure and add the traces\n",
    "fig = go.Figure(data=[trace_all, trace1, trace2, trace3], layout=layout)\n",
    "\n",
    "# Update x-axis range\n",
    "fig.update_xaxes(range=['2019-04-10', '2019-10-30'])\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM method - Multivariate imputation using darts\n",
    "\n",
    "# Initialize the transformer and the model\n",
    "transformer = Scaler()\n",
    "model = RNNModel(\n",
    "    model=\"LSTM\",\n",
    "    hidden_dim=20,\n",
    "    dropout=0,\n",
    "    batch_size=16,\n",
    "    n_epochs=50,\n",
    "    optimizer_kwargs={\"lr\": 1e-3},\n",
    "    random_state=0,\n",
    "    training_length=50,\n",
    "    input_chunk_length=20,\n",
    "    likelihood=GaussianLikelihood(),\n",
    ")\n",
    "# Prepare the data\n",
    "data1 = data.copy()\n",
    "data1=data1.reset_index()\n",
    "data1['TIME1'] = pd.to_datetime(data1['TIME1'])\n",
    "data1 = data1[data1['TIME1'] <= '2019-06-11']\n",
    "data2 = data.copy()\n",
    "data2=data2.reset_index()\n",
    "data2['TIME1'] = pd.to_datetime(data2['TIME1'])\n",
    "data2 = data2.drop_duplicates(subset='TIME1')\n",
    "data_ts = TimeSeries.from_dataframe(data1, 'TIME1', ['PED', 'AQI', #'Daily Case Rate / 100,000',\n",
    "                                                     'Average Temp', 'Precipitation'], fill_missing_dates=True, freq='D')\n",
    "data_scaled = transformer.fit_transform(data_ts)\n",
    "\n",
    "# Create year and month covariate series\n",
    "year_series = datetime_attribute_timeseries(\n",
    "    pd.date_range(start=data_ts.start_time(), freq=data_ts.freq_str, periods=len(data_ts)+91),\n",
    "    attribute=\"year\",\n",
    "    one_hot=False,\n",
    ")\n",
    "year_series = Scaler().fit_transform(year_series)\n",
    "month_series = datetime_attribute_timeseries(\n",
    "    year_series, attribute=\"month\", one_hot=True\n",
    ")\n",
    "covariates = year_series.stack(month_series)\n",
    "\n",
    "# Create future covariates TimeSeries\n",
    "future_covariates = TimeSeries.from_dataframe(data2, 'TIME1', ['AQI', 'Average Temp', 'Precipitation'], fill_missing_dates=True, freq='D')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(data_scaled, future_covariates=future_covariates)\n",
    "\n",
    "# Predict the next n days\n",
    "forecast = model.predict(n=91, future_covariates=future_covariates)\n",
    "\n",
    "# Re-scale the values to the original scale\n",
    "forecast = transformer.inverse_transform(forecast)\n",
    "forecast_PED = forecast['PED']\n",
    "\n",
    "# Plot the forecast\n",
    "data_ts['PED'].plot(label='actual')\n",
    "forecast_PED.plot(label='forecast')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRU method - Univariate imputation using darts\n",
    "\n",
    "# Initialize the transformer and the model\n",
    "transformer = Scaler()\n",
    "model = RNNModel(\n",
    "    model=\"GRU\",\n",
    "    hidden_dim=20,\n",
    "    dropout=0,\n",
    "    batch_size=16,\n",
    "    n_epochs=50,\n",
    "    optimizer_kwargs={\"lr\": 1e-3},\n",
    "    random_state=0,\n",
    "    training_length=50,\n",
    "    input_chunk_length=20,\n",
    "    likelihood=GaussianLikelihood(),\n",
    ")\n",
    "# Prepare the data\n",
    "data1 = data.copy()\n",
    "data1=data1.reset_index()\n",
    "data1['TIME1'] = pd.to_datetime(data1['TIME1'])\n",
    "data1 = data1[data1['TIME1'] <= '2019-06-11']\n",
    "data2 = data.copy()\n",
    "data2=data2.reset_index()\n",
    "data2['TIME1'] = pd.to_datetime(data2['TIME1'])\n",
    "data2 = data2[data2['TIME1'] >= '2019-09-10']\n",
    "data2.set_index('TIME1', inplace=True)\n",
    "data_ts = TimeSeries.from_dataframe(data1, 'TIME1', 'PED')\n",
    "data_scaled = transformer.fit_transform(data_ts)\n",
    "\n",
    "# Create year and month covariate series\n",
    "year_series = datetime_attribute_timeseries(\n",
    "    pd.date_range(start=data_ts.start_time(), freq=data_ts.freq_str, periods=len(data_ts)+91),\n",
    "    attribute=\"year\",\n",
    "    one_hot=False,\n",
    ")\n",
    "year_series = Scaler().fit_transform(year_series)\n",
    "month_series = datetime_attribute_timeseries(\n",
    "    year_series, attribute=\"month\", one_hot=True\n",
    ")\n",
    "covariates = year_series.stack(month_series)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(data_scaled, future_covariates=covariates)\n",
    "\n",
    "# Predict the next n days\n",
    "forecast = model.predict(n=91, future_covariates=covariates)\n",
    "\n",
    "# Re-scale the values to the original scale\n",
    "forecast = transformer.inverse_transform(forecast)\n",
    "\n",
    "# Create a single trace for the entire dataset\n",
    "trace_all = go.Scatter(\n",
    "    x=np.concatenate([data_ts.time_index, forecast.time_index, data2.index]),\n",
    "    y=np.concatenate([data_ts.univariate_values(), forecast.univariate_values(), data2['PED'].values]),\n",
    "    mode='lines',\n",
    "    name='All',\n",
    "    line=dict(color='red'),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Create separate traces for each segment with desired colors\n",
    "trace1 = go.Scatter(\n",
    "    x=data_ts.time_index, \n",
    "    y=data_ts.univariate_values(),\n",
    "    mode='lines',\n",
    "    line=dict(color='#636EFA'),\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x=forecast.time_index, \n",
    "    y=forecast.univariate_values(),\n",
    "    mode='lines',\n",
    "    name='Imputed data',\n",
    "    line=dict(color='red'),\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "    x=data2.index,\n",
    "    y=data2['PED'],\n",
    "    mode='lines',\n",
    "    line=dict(color='#636EFA'),\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='Pedestrian volume'),\n",
    ")\n",
    "\n",
    "# Create the figure and add the traces\n",
    "fig = go.Figure(data=[trace_all, trace1, trace2, trace3], layout=layout)\n",
    "\n",
    "# Update x-axis range\n",
    "fig.update_xaxes(range=['2019-04-10', '2019-10-30'])\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRU method - Multivariate imputation using darts\n",
    "\n",
    "# Initialize the transformer and the model\n",
    "transformer = Scaler()\n",
    "model = RNNModel(\n",
    "    model=\"GRU\",\n",
    "    hidden_dim=20,\n",
    "    dropout=0,\n",
    "    batch_size=16,\n",
    "    n_epochs=50,\n",
    "    optimizer_kwargs={\"lr\": 1e-3},\n",
    "    random_state=0,\n",
    "    training_length=50,\n",
    "    input_chunk_length=20,\n",
    "    likelihood=GaussianLikelihood(),\n",
    ")\n",
    "# Prepare the data\n",
    "data1 = data.copy()\n",
    "data1=data1.reset_index()\n",
    "data1['TIME1'] = pd.to_datetime(data1['TIME1'])\n",
    "data1 = data1[data1['TIME1'] <= '2019-06-11']\n",
    "data2 = data.copy()\n",
    "data2=data2.reset_index()\n",
    "data2['TIME1'] = pd.to_datetime(data2['TIME1'])\n",
    "data2 = data2.drop_duplicates(subset='TIME1')\n",
    "data_ts = TimeSeries.from_dataframe(data1, 'TIME1', ['PED', 'AQI', #'Daily Case Rate / 100,000',\n",
    "                                                     'Average Temp', 'Precipitation'], fill_missing_dates=True, freq='D')\n",
    "data_scaled = transformer.fit_transform(data_ts)\n",
    "\n",
    "# Create year and month covariate series\n",
    "year_series = datetime_attribute_timeseries(\n",
    "    pd.date_range(start=data_ts.start_time(), freq=data_ts.freq_str, periods=len(data_ts)+91),\n",
    "    attribute=\"year\",\n",
    "    one_hot=False,\n",
    ")\n",
    "year_series = Scaler().fit_transform(year_series)\n",
    "month_series = datetime_attribute_timeseries(\n",
    "    year_series, attribute=\"month\", one_hot=True\n",
    ")\n",
    "covariates = year_series.stack(month_series)\n",
    "\n",
    "# Create future covariates TimeSeries\n",
    "future_covariates = TimeSeries.from_dataframe(data2, 'TIME1', ['AQI', 'Average Temp', 'Precipitation'], fill_missing_dates=True, freq='D')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(data_scaled, future_covariates=future_covariates)\n",
    "\n",
    "# Predict the next n days\n",
    "forecast = model.predict(n=91, future_covariates=future_covariates)\n",
    "\n",
    "# Re-scale the values to the original scale\n",
    "forecast = transformer.inverse_transform(forecast)\n",
    "forecast_PED = forecast['PED']\n",
    "\n",
    "# Plot the forecast\n",
    "data_ts['PED'].plot(label='actual')\n",
    "forecast_PED.plot(label='forecast')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN method - Univariate imputation using darts\n",
    "\n",
    "# Initialize the transformer and the model\n",
    "transformer = Scaler()\n",
    "model = RNNModel(\n",
    "    model=\"RNN\",\n",
    "    hidden_dim=20,\n",
    "    dropout=0,\n",
    "    batch_size=16,\n",
    "    n_epochs=50,\n",
    "    optimizer_kwargs={\"lr\": 1e-3},\n",
    "    random_state=0,\n",
    "    training_length=50,\n",
    "    input_chunk_length=20,\n",
    "    likelihood=GaussianLikelihood(),\n",
    ")\n",
    "# Prepare the data\n",
    "data1 = data.copy()\n",
    "data1=data1.reset_index()\n",
    "data1['TIME1'] = pd.to_datetime(data1['TIME1'])\n",
    "data1 = data1[data1['TIME1'] <= '2019-06-11']\n",
    "data2 = data.copy()\n",
    "data2=data2.reset_index()\n",
    "data2['TIME1'] = pd.to_datetime(data2['TIME1'])\n",
    "data2 = data2[data2['TIME1'] >= '2019-09-10']\n",
    "data2.set_index('TIME1', inplace=True)\n",
    "data_ts = TimeSeries.from_dataframe(data1, 'TIME1', 'PED')\n",
    "data_scaled = transformer.fit_transform(data_ts)\n",
    "\n",
    "# Create year and month covariate series\n",
    "year_series = datetime_attribute_timeseries(\n",
    "    pd.date_range(start=data_ts.start_time(), freq=data_ts.freq_str, periods=len(data_ts)+91),\n",
    "    attribute=\"year\",\n",
    "    one_hot=False,\n",
    ")\n",
    "year_series = Scaler().fit_transform(year_series)\n",
    "month_series = datetime_attribute_timeseries(\n",
    "    year_series, attribute=\"month\", one_hot=True\n",
    ")\n",
    "covariates = year_series.stack(month_series)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(data_scaled, future_covariates=covariates)\n",
    "\n",
    "# Predict the next n days\n",
    "forecast = model.predict(n=91, future_covariates=covariates)\n",
    "\n",
    "# Re-scale the values to the original scale\n",
    "forecast = transformer.inverse_transform(forecast)\n",
    "\n",
    "# Create a single trace for the entire dataset\n",
    "trace_all = go.Scatter(\n",
    "    x=np.concatenate([data_ts.time_index, forecast.time_index, data2.index]),\n",
    "    y=np.concatenate([data_ts.univariate_values(), forecast.univariate_values(), data2['PED'].values]),\n",
    "    mode='lines',\n",
    "    name='All',\n",
    "    line=dict(color='red'),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Create separate traces for each segment with desired colors\n",
    "trace1 = go.Scatter(\n",
    "    x=data_ts.time_index, \n",
    "    y=data_ts.univariate_values(),\n",
    "    mode='lines',\n",
    "    line=dict(color='#636EFA'),\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x=forecast.time_index, \n",
    "    y=forecast.univariate_values(),\n",
    "    mode='lines',\n",
    "    name='Imputed data',\n",
    "    line=dict(color='red'),\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "    x=data2.index,\n",
    "    y=data2['PED'],\n",
    "    mode='lines',\n",
    "    line=dict(color='#636EFA'),\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='Pedestrian volume'),\n",
    ")\n",
    "\n",
    "# Create the figure and add the traces\n",
    "fig = go.Figure(data=[trace_all, trace1, trace2, trace3], layout=layout)\n",
    "\n",
    "# Update x-axis range\n",
    "fig.update_xaxes(range=['2019-04-10', '2019-10-30'])\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN method - Multivariate imputation using darts\n",
    "\n",
    "# Initialize the transformer and the model\n",
    "transformer = Scaler()\n",
    "model = RNNModel(\n",
    "    model=\"RNN\",\n",
    "    hidden_dim=20,\n",
    "    dropout=0,\n",
    "    batch_size=16,\n",
    "    n_epochs=50,\n",
    "    optimizer_kwargs={\"lr\": 1e-3},\n",
    "    random_state=0,\n",
    "    training_length=50,\n",
    "    input_chunk_length=20,\n",
    "    likelihood=GaussianLikelihood(),\n",
    ")\n",
    "# Prepare the data\n",
    "data1 = data.copy()\n",
    "data1=data1.reset_index()\n",
    "data1['TIME1'] = pd.to_datetime(data1['TIME1'])\n",
    "data1 = data1[data1['TIME1'] <= '2019-06-11']\n",
    "data2 = data.copy()\n",
    "data2=data2.reset_index()\n",
    "data2['TIME1'] = pd.to_datetime(data2['TIME1'])\n",
    "data2 = data2.drop_duplicates(subset='TIME1')\n",
    "data_ts = TimeSeries.from_dataframe(data1, 'TIME1', ['PED', 'AQI', #'Daily Case Rate / 100,000',\n",
    "                                                     'Average Temp', 'Precipitation'], fill_missing_dates=True, freq='D')\n",
    "data_scaled = transformer.fit_transform(data_ts)\n",
    "\n",
    "# Create year and month covariate series\n",
    "year_series = datetime_attribute_timeseries(\n",
    "    pd.date_range(start=data_ts.start_time(), freq=data_ts.freq_str, periods=len(data_ts)+91),\n",
    "    attribute=\"year\",\n",
    "    one_hot=False,\n",
    ")\n",
    "year_series = Scaler().fit_transform(year_series)\n",
    "month_series = datetime_attribute_timeseries(\n",
    "    year_series, attribute=\"month\", one_hot=True\n",
    ")\n",
    "covariates = year_series.stack(month_series)\n",
    "\n",
    "# Create future covariates TimeSeries\n",
    "future_covariates = TimeSeries.from_dataframe(data2, 'TIME1', ['AQI', 'Average Temp', 'Precipitation'], fill_missing_dates=True, freq='D')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(data_scaled, future_covariates=future_covariates)\n",
    "\n",
    "# Predict the next n days\n",
    "forecast = model.predict(n=91, future_covariates=future_covariates)\n",
    "\n",
    "# Re-scale the values to the original scale\n",
    "forecast = transformer.inverse_transform(forecast)\n",
    "forecast_PED = forecast['PED']\n",
    "\n",
    "# Plot the forecast\n",
    "data_ts['PED'].plot(label='actual')\n",
    "forecast_PED.plot(label='forecast')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TCN method - Univariate imputation using darts\n",
    "\n",
    "# Initialize the transformer and the model\n",
    "transformer = Scaler()\n",
    "model = TCNModel(\n",
    "    input_chunk_length=13,\n",
    "    output_chunk_length=12,\n",
    "    n_epochs=500,\n",
    "    dropout=0.1,\n",
    "    dilation_base=2,\n",
    "    weight_norm=True,\n",
    "    kernel_size=5,\n",
    "    num_filters=3,\n",
    "    random_state=0,\n",
    ")\n",
    "# Prepare the data\n",
    "data1 = data.copy()\n",
    "data1=data1.reset_index()\n",
    "data1['TIME1'] = pd.to_datetime(data1['TIME1'])\n",
    "data1 = data1[data1['TIME1'] <= '2019-06-11']\n",
    "data2 = data.copy()\n",
    "data2=data2.reset_index()\n",
    "data2['TIME1'] = pd.to_datetime(data2['TIME1'])\n",
    "data2 = data2[data2['TIME1'] >= '2019-09-10']\n",
    "data2.set_index('TIME1', inplace=True)\n",
    "data_ts = TimeSeries.from_dataframe(data1, 'TIME1', 'PED')\n",
    "data_scaled = transformer.fit_transform(data_ts)\n",
    "\n",
    "# Create year and month covariate series\n",
    "year_series = datetime_attribute_timeseries(\n",
    "    pd.date_range(start=data_ts.start_time(), freq=data_ts.freq_str, periods=len(data_ts)+91),\n",
    "    attribute=\"year\",\n",
    "    one_hot=False,\n",
    ")\n",
    "year_series = Scaler().fit_transform(year_series)\n",
    "month_series = datetime_attribute_timeseries(\n",
    "    year_series, attribute=\"month\", one_hot=True\n",
    ")\n",
    "covariates = year_series.stack(month_series)\n",
    "\n",
    "# Fit the model for the n next days\n",
    "model.fit(data_scaled, past_covariates=covariates)\n",
    "forecast = model.predict(n=91, past_covariates=covariates)\n",
    "\n",
    "# Re-scale the values to the original scale\n",
    "forecast = transformer.inverse_transform(forecast)\n",
    "\n",
    "# Create a single trace for the entire dataset\n",
    "trace_all = go.Scatter(\n",
    "    x=np.concatenate([data_ts.time_index, forecast.time_index, data2.index]),\n",
    "    y=np.concatenate([data_ts.univariate_values(), forecast.univariate_values(), data2['PED'].values]),\n",
    "    mode='lines',\n",
    "    name='All',\n",
    "    line=dict(color='red'),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Create separate traces for each segment with desired colors\n",
    "trace1 = go.Scatter(\n",
    "    x=data_ts.time_index, \n",
    "    y=data_ts.univariate_values(),\n",
    "    mode='lines',\n",
    "    line=dict(color='#636EFA'),\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x=forecast.time_index, \n",
    "    y=forecast.univariate_values(),\n",
    "    mode='lines',\n",
    "    name='Imputed data',\n",
    "    line=dict(color='red'),\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "    x=data2.index,\n",
    "    y=data2['PED'],\n",
    "    mode='lines',\n",
    "    line=dict(color='#636EFA'),\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='Pedestrian volume'),\n",
    ")\n",
    "\n",
    "# Create the figure and add the traces\n",
    "fig = go.Figure(data=[trace_all, trace1, trace2, trace3], layout=layout)\n",
    "\n",
    "# Update x-axis range\n",
    "fig.update_xaxes(range=['2019-04-10', '2019-10-30'])\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TCN method - Multivariate imputation using darts\n",
    "\n",
    "# Initialize the transformer and the model\n",
    "transformer = Scaler()\n",
    "model = TCNModel(\n",
    "    input_chunk_length=13,\n",
    "    output_chunk_length=12,\n",
    "    n_epochs=500,\n",
    "    dropout=0.1,\n",
    "    dilation_base=2,\n",
    "    weight_norm=True,\n",
    "    kernel_size=5,\n",
    "    num_filters=3,\n",
    "    random_state=0,\n",
    ")\n",
    "# Prepare the data\n",
    "data1 = data.copy()\n",
    "data1=data1.reset_index()\n",
    "data1['TIME1'] = pd.to_datetime(data1['TIME1'])\n",
    "data1 = data1[data1['TIME1'] <= '2019-06-11']\n",
    "data2 = data.copy()\n",
    "data2=data2.reset_index()\n",
    "data2['TIME1'] = pd.to_datetime(data2['TIME1'])\n",
    "data2 = data2.drop_duplicates(subset='TIME1')\n",
    "data_ts = TimeSeries.from_dataframe(data1, 'TIME1', ['PED', 'AQI', #'Daily Case Rate / 100,000',\n",
    "                                                     'Average Temp', 'Precipitation'], fill_missing_dates=True, freq='D')\n",
    "data_scaled = transformer.fit_transform(data_ts)\n",
    "\n",
    "# Create year and month covariate series\n",
    "year_series = datetime_attribute_timeseries(\n",
    "    pd.date_range(start=data_ts.start_time(), freq=data_ts.freq_str, periods=len(data_ts)+91),\n",
    "    attribute=\"year\",\n",
    "    one_hot=False,\n",
    ")\n",
    "year_series = Scaler().fit_transform(year_series)\n",
    "month_series = datetime_attribute_timeseries(\n",
    "    year_series, attribute=\"month\", one_hot=True\n",
    ")\n",
    "covariates = year_series.stack(month_series)\n",
    "\n",
    "# Fit the model for n next days\n",
    "model.fit(data_scaled, past_covariates=covariates)\n",
    "forecast = model.predict(n=91, past_covariates=covariates)\n",
    "\n",
    "# Re-scale the values to the original scale\n",
    "forecast = transformer.inverse_transform(forecast)\n",
    "forecast_PED = forecast['PED']\n",
    "\n",
    "# Plot the forecast\n",
    "data_ts['PED'].plot(label='actual')\n",
    "forecast_PED.plot(label='forecast')\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
